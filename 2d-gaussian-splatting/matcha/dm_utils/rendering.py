import math
import numpy as np
import torch
from pytorch3d.transforms.transform3d import Transform3d


C0 = 0.28209479177387814
C1 = 0.4886025119029199
C2 = [
    1.0925484305920792,
    -1.0925484305920792,
    0.31539156525252005,
    -1.0925484305920792,
    0.5462742152960396
]
C3 = [
    -0.5900435899266435,
    2.890611442640554,
    -0.4570457994644658,
    0.3731763325901154,
    -0.4570457994644658,
    1.445305721320277,
    -0.5900435899266435
]
C4 = [
    2.5033429417967046,
    -1.7701307697799304,
    0.9461746957575601,
    -0.6690465435572892,
    0.10578554691520431,
    -0.6690465435572892,
    0.47308734787878004,
    -1.7701307697799304,
    0.6258357354491761,
]   


def eval_sh(deg, sh, dirs):
    """
    Evaluate spherical harmonics at unit directions
    using hardcoded SH polynomials.
    Works with torch/np/jnp.
    ... Can be 0 or more batch dimensions.
    Args:
        deg: int SH deg. Currently, 0-3 supported
        sh: jnp.ndarray SH coeffs [..., C, (deg + 1) ** 2]
        dirs: jnp.ndarray unit directions [..., 3]
    Returns:
        [..., C]
    """
    assert deg <= 4 and deg >= 0
    coeff = (deg + 1) ** 2
    assert sh.shape[-1] >= coeff

    result = C0 * sh[..., 0]
    if deg > 0:
        x, y, z = dirs[..., 0:1], dirs[..., 1:2], dirs[..., 2:3]
        result = (result -
                C1 * y * sh[..., 1] +
                C1 * z * sh[..., 2] -
                C1 * x * sh[..., 3])

        if deg > 1:
            xx, yy, zz = x * x, y * y, z * z
            xy, yz, xz = x * y, y * z, x * z
            result = (result +
                    C2[0] * xy * sh[..., 4] +
                    C2[1] * yz * sh[..., 5] +
                    C2[2] * (2.0 * zz - xx - yy) * sh[..., 6] +
                    C2[3] * xz * sh[..., 7] +
                    C2[4] * (xx - yy) * sh[..., 8])

            if deg > 2:
                result = (result +
                C3[0] * y * (3 * xx - yy) * sh[..., 9] +
                C3[1] * xy * z * sh[..., 10] +
                C3[2] * y * (4 * zz - xx - yy)* sh[..., 11] +
                C3[3] * z * (2 * zz - 3 * xx - 3 * yy) * sh[..., 12] +
                C3[4] * x * (4 * zz - xx - yy) * sh[..., 13] +
                C3[5] * z * (xx - yy) * sh[..., 14] +
                C3[6] * x * (xx - 3 * yy) * sh[..., 15])

                if deg > 3:
                    result = (result + C4[0] * xy * (xx - yy) * sh[..., 16] +
                            C4[1] * yz * (3 * xx - yy) * sh[..., 17] +
                            C4[2] * xy * (7 * zz - 1) * sh[..., 18] +
                            C4[3] * yz * (7 * zz - 3) * sh[..., 19] +
                            C4[4] * (zz * (35 * zz - 30) + 3) * sh[..., 20] +
                            C4[5] * xz * (7 * zz - 3) * sh[..., 21] +
                            C4[6] * (xx - yy) * (7 * zz - 1) * sh[..., 22] +
                            C4[7] * xz * (xx - 3 * yy) * sh[..., 23] +
                            C4[8] * (xx * (xx - 3 * yy) - yy * (3 * xx - yy)) * sh[..., 24])
    return result


def RGB2SH(rgb):
    return (rgb - 0.5) / C0


def SH2RGB(sh):
    return sh * C0 + 0.5


def getWorld2View2(R, t, translate=torch.tensor([0.0, 0.0, 0.0]), scale=1.0):
    if type(R) == np.ndarray:
        _translate = translate.cpu().numpy()
        Rt = np.zeros((4, 4))
        Rt[:3, :3] = R.transpose()
        Rt[:3, 3] = t
        Rt[3, 3] = 1.0

        C2W = np.linalg.inv(Rt)
        cam_center = C2W[:3, 3]
        cam_center = (cam_center + _translate) * scale
        C2W[:3, 3] = cam_center
        Rt = np.linalg.inv(C2W)
        return np.float32(Rt)
    else:
        translate = translate.to(R.device)
        Rt = torch.zeros((4, 4), device=R.device)
        Rt[:3, :3] = R.transpose(-1, -2)
        Rt[:3, 3] = t
        Rt[3, 3] = 1.0

        C2W = torch.linalg.inv(Rt)
        cam_center = C2W[:3, 3]
        cam_center = (cam_center + translate) * scale
        C2W[:3, 3] = cam_center
        Rt = torch.linalg.inv(C2W)
    return Rt


def getProjectionMatrix(znear, zfar, fovX, fovY):
    if isinstance(fovX, torch.Tensor):
        tanHalfFovX = torch.tan((fovX / 2))
    else:
        tanHalfFovX = math.tan((fovX / 2))
    
    if isinstance(fovY, torch.Tensor):
        tanHalfFovY = torch.tan((fovY / 2))
    else:
        tanHalfFovY = math.tan((fovY / 2))
    

    top = tanHalfFovY * znear
    bottom = -top
    right = tanHalfFovX * znear
    left = -right

    if isinstance(fovX, torch.Tensor):
        P = torch.zeros(4, 4, device=fovX.device)
    else:
        P = torch.zeros(4, 4)

    z_sign = 1.0

    P[0, 0] = 2.0 * znear / (right - left)
    P[1, 1] = 2.0 * znear / (top - bottom)
    P[0, 2] = (right + left) / (right - left)
    P[1, 2] = (top + bottom) / (top - bottom)
    P[3, 2] = z_sign
    P[2, 2] = z_sign * zfar / (zfar - znear)
    P[2, 3] = -(zfar * znear) / (zfar - znear)
    return P


def fov2focal(fov, pixels):
    if isinstance(fov, torch.Tensor) or isinstance(pixels, torch.Tensor):
        return pixels / (2 * torch.tan(fov / 2))
    else:
        return pixels / (2 * math.tan(fov / 2))


def focal2fov(focal, pixels):
    if isinstance(focal, torch.Tensor) or isinstance(pixels, torch.Tensor):
        return 2 * torch.atan(pixels / (2 * focal))
    else:
        return 2 * math.atan(pixels / (2 * focal))


def inverse_sigmoid(x):
    return torch.log(x/(1-x))


def convert_normal_render_to_01(
    normal_img:torch.Tensor,
    transform:Transform3d=None,
    ):
    """Convert normal image to [0, 1] range.
    A 3D transformation can be applied to the normals before conversion.

    Args:
        normal_img (torch.Tensor): Has shape (height, width, 3).
        transform (Transform3d, optional): _description_. Defaults to None.

    Returns:
        _type_: _description_
    """
    if transform:
        _normal_img = transform.transform_normals(normal_img.view(-1, 3)).view(normal_img.shape)
    else:
        _normal_img = normal_img
    
    return (1. - _normal_img) / 2.


def depths_to_points(view, depthmap):
    """Comes from 2DGS.

    Args:
        view (_type_): _description_
        depthmap (_type_): _description_

    Returns:
        _type_: _description_
    """
    c2w = (view.world_view_transform.T).inverse()
    W, H = view.image_width, view.image_height
    ndc2pix = torch.tensor([
        [W / 2, 0, 0, (W) / 2],
        [0, H / 2, 0, (H) / 2],
        [0, 0, 0, 1]]).float().cuda().T
    projection_matrix = c2w.T @ view.full_proj_transform
    intrins = (projection_matrix @ ndc2pix)[:3,:3].T
    
    grid_x, grid_y = torch.meshgrid(torch.arange(W, device='cuda').float(), torch.arange(H, device='cuda').float(), indexing='xy')
    points = torch.stack([grid_x, grid_y, torch.ones_like(grid_x)], dim=-1).reshape(-1, 3)
    rays_d = points @ intrins.inverse().T @ c2w[:3,:3].T
    rays_o = c2w[:3,3]
    points = depthmap.reshape(-1, 1) * rays_d + rays_o
    return points


def depth2normal_2dgs(view, depth):
    """Comes from 2DGS.
    
        view: view camera
        depth: depthmap. Has shape (1, H, W).
    """
    points = depths_to_points(view, depth).reshape(*depth.shape[1:], 3)
    output = torch.zeros_like(points)
    dx = torch.cat([points[2:, 1:-1] - points[:-2, 1:-1]], dim=0)
    dy = torch.cat([points[1:-1, 2:] - points[1:-1, :-2]], dim=1)
    normal_map = torch.nn.functional.normalize(torch.cross(dx, dy, dim=-1), dim=-1)
    output[1:-1, 1:-1, :] = normal_map
    return output


def depths_to_points_parallel(
    depthmap,
    cameras=None,
    world_view_transforms=None, 
    full_proj_transforms=None,
):
    """Reworked. Originally comes from 2DGS.

    Args:
        world_view_transforms (_type_): (n_camera, 4, 4)
        full_proj_transforms (_type_): (n_camera, 4, 4)
        depthmap (_type_): (n_camera, H, W) or (n_camera, 1, H, W)

    Returns:
        _type_: _description_
    """
    no_matrix_provided = (world_view_transforms is None) or (full_proj_transforms is None)
    if no_matrix_provided and cameras is None:
        raise ValueError("Either provide the camera matrices or the camera objects.")
    if world_view_transforms is None:
        world_view_transforms = torch.stack([gs_camera.world_view_transform for gs_camera in cameras.gs_cameras])
    if full_proj_transforms is None:
        full_proj_transforms = torch.stack([gs_camera.full_proj_transform for gs_camera in cameras.gs_cameras])
        # full_proj_transforms = torch.stack([cameras.gs_cameras[i].full_proj_transform for i in range(len(cameras))])
    
    c2w = (world_view_transforms.transpose(-1, -2)).inverse()  # (n_camera, 4, 4)
    W, H = depthmap.shape[-1], depthmap.shape[-2]
    ndc2pix = torch.tensor([
        [W / 2, 0, 0, (W) / 2],
        [0, H / 2, 0, (H) / 2],
        [0, 0, 0, 1]]).float().cuda().T  # (4, 3)
    projection_matrix = c2w.transpose(-1, -2) @ full_proj_transforms  # (n_camera, 4, 4)
    intrins = (projection_matrix @ ndc2pix)[..., :3,:3].transpose(-1, -2)  # (n_camera, 3, 3)
    
    grid_x, grid_y = torch.meshgrid(torch.arange(W, device='cuda').float(), torch.arange(H, device='cuda').float(), indexing='xy')  # (H, W)
    points = torch.stack([grid_x, grid_y, torch.ones_like(grid_x)], dim=-1).reshape(-1, 3)  # (H * W, 3)
    rays_d = points[None] @ intrins.inverse().transpose(-1, -2) @ c2w[..., :3,:3].transpose(-1, -2)  # (n_camera, H * W, 3)
    rays_o = c2w[..., None, :3,3]  # (n_camera, 3)
    points = depthmap.reshape(-1, H*W, 1) * rays_d + rays_o
    return points


def depth2normal_parallel(
    depths,
    cameras=None,
    world_view_transforms=None, 
    full_proj_transforms=None, 
):
    """Reworked. Originally comes from 2DGS.
    
        view: view camera
        depth: depthmap 
    """
    
    no_matrix_provided = (world_view_transforms is None) or (full_proj_transforms is None)
    if no_matrix_provided and cameras is None:
        raise ValueError("Either provide the camera matrices or the camera objects.")
    if world_view_transforms is None:
        world_view_transforms = torch.stack([cameras.gs_cameras[i].world_view_transform for i in range(len(cameras))])
    if full_proj_transforms is None:
        full_proj_transforms = torch.stack([cameras.gs_cameras[i].full_proj_transform for i in range(len(cameras))])
    
    height, width = depths.shape[-2:]
    points = depths_to_points_parallel(
        world_view_transforms=world_view_transforms, 
        full_proj_transforms=full_proj_transforms, 
        depthmap=depths,
    ).reshape(-1, height, width, 3)
    output = torch.zeros_like(points)
    dx = torch.cat([points[:, 2:, 1:-1] - points[:, :-2, 1:-1]], dim=1)
    dy = torch.cat([points[:, 1:-1, 2:] - points[:, 1:-1, :-2]], dim=2)
    normal_map = torch.nn.functional.normalize(torch.cross(dx, dy, dim=-1), dim=-1)
    output[..., 1:-1, 1:-1, :] = normal_map
    return output


def depth2normal_surfel(depth, mask, camera):
    """Comes from Gaussian Surfels.

    Args:
        depth (_type_): _description_
        mask (_type_): _description_
        camera (_type_): _description_

    Returns:
        _type_: _description_
    """
    # conver to camera position
    camD = depth.permute([1, 2, 0])
    mask = mask.permute([1, 2, 0])
    shape = camD.shape
    device = camD.device
    h, w, _ = torch.meshgrid(torch.arange(0, shape[0]), torch.arange(0, shape[1]), torch.arange(0, shape[2]), indexing='ij')
    # print(h)
    h = h.to(torch.float32).to(device)
    w = w.to(torch.float32).to(device)
    p = torch.cat([w, h], axis=-1)
    
    p[..., 0:1] -= camera.prcppoint[0] * camera.image_width
    p[..., 1:2] -= camera.prcppoint[1] * camera.image_height
    p *= camD
    K00 = fov2focal(camera.FoVy, camera.image_height)
    K11 = fov2focal(camera.FoVx, camera.image_width)
    K = torch.tensor([K00, 0, 0, K11]).reshape([2,2])
    Kinv = torch.inverse(K).to(device)
    # print(p.shape, Kinv.shape)
    p = p @ Kinv.t()
    camPos = torch.cat([p, camD], -1)

    # padded = mod.contour_padding(camPos.contiguous(), mask.contiguous(), torch.zeros_like(camPos), filter_size // 2)
    # camPos = camPos + padded
    p = torch.nn.functional.pad(camPos[None], [0, 0, 1, 1, 1, 1], mode='replicate')
    mask = torch.nn.functional.pad(mask[None].to(torch.float32), [0, 0, 1, 1, 1, 1], mode='replicate').to(torch.bool)
    

    p_c = (p[:, 1:-1, 1:-1, :]      ) * mask[:, 1:-1, 1:-1, :]
    p_u = (p[:,  :-2, 1:-1, :] - p_c) * mask[:,  :-2, 1:-1, :]
    p_l = (p[:, 1:-1,  :-2, :] - p_c) * mask[:, 1:-1,  :-2, :]
    p_b = (p[:, 2:  , 1:-1, :] - p_c) * mask[:, 2:  , 1:-1, :]
    p_r = (p[:, 1:-1, 2:  , :] - p_c) * mask[:, 1:-1, 2:  , :]

    n_ul = torch.cross(p_u, p_l)
    n_ur = torch.cross(p_r, p_u)
    n_br = torch.cross(p_b, p_r)
    n_bl = torch.cross(p_l, p_b)
    
    n = n_ul + n_ur + n_br + n_bl
    n = n[0]
    
    # n *= -torch.sum(camVDir * camN, -1, True).sign() # no cull back

    mask = mask[0, 1:-1, 1:-1, :]

    # n = gaussian_blur(n, filter_size, 1) * mask

    n = torch.nn.functional.normalize(n, dim=-1)
    # n[..., 1] *= -1
    # n *= -1

    n = (n * mask).permute([2, 0, 1])
    return n


def normal2curv(normal, mask):
    # normal = normal.detach()
    n = normal.permute([1, 2, 0])
    m = mask.permute([1, 2, 0])
    n = torch.nn.functional.pad(n[None], [0, 0, 1, 1, 1, 1], mode='replicate')
    m = torch.nn.functional.pad(m[None].to(torch.float32), [0, 0, 1, 1, 1, 1], mode='replicate').to(torch.bool)
    n_c = (n[:, 1:-1, 1:-1, :]      ) * m[:, 1:-1, 1:-1, :]
    n_u = (n[:,  :-2, 1:-1, :] - n_c) * m[:,  :-2, 1:-1, :]
    n_l = (n[:, 1:-1,  :-2, :] - n_c) * m[:, 1:-1,  :-2, :]
    n_b = (n[:, 2:  , 1:-1, :] - n_c) * m[:, 2:  , 1:-1, :]
    n_r = (n[:, 1:-1, 2:  , :] - n_c) * m[:, 1:-1, 2:  , :]
    curv = (n_u + n_l + n_b + n_r)[0]
    curv = curv.permute([2, 0, 1]) * mask
    curv = curv.norm(1, 0, True)
    return curv


def normal2curv_parallel(normal, mask):
    """Reworked. Originally comes from Gaussian Surfels.
    
    Args:
        normal (torch.Tensor): (n_camera, H, W, 3) or (n_camera, 3, H, W)
        mask (torch.Tensor): (n_camera, H, W, 1) or (n_camera, 1, H, W)
    """
    # normal = normal.detach()
    if normal.shape[-1] != 3:
        n = normal.permute(0, -2, -1, -3)
        m = mask.permute(0, -2, -1, -3)
    else:
        n = normal.clone()
        m = mask.clone()
    n = torch.nn.functional.pad(n[:, None], [0, 0, 1, 1, 1, 1], mode='replicate')
    m = torch.nn.functional.pad(m[:, None].to(torch.float32), [0, 0, 1, 1, 1, 1], mode='replicate').to(torch.bool)
    n_c = (n[..., 1:-1, 1:-1, :]      ) * m[..., 1:-1, 1:-1, :]
    n_u = (n[...,  :-2, 1:-1, :] - n_c) * m[...,  :-2, 1:-1, :]
    n_l = (n[..., 1:-1,  :-2, :] - n_c) * m[..., 1:-1,  :-2, :]
    n_b = (n[..., 2:  , 1:-1, :] - n_c) * m[..., 2:  , 1:-1, :]
    n_r = (n[..., 1:-1, 2:  , :] - n_c) * m[..., 1:-1, 2:  , :]
    curv = (n_u + n_l + n_b + n_r)[:, 0]
    curv = curv.norm(p=1, dim=-1, keepdim=True)
    if normal.shape[-1] != 3:
        curv = curv.permute(0, -1, -3, -2) * mask
    else:
        curv = curv * mask
    return curv


def skew_sym_mat(x):
    device = x.device
    dtype = x.dtype
    ssm = torch.zeros(3, 3, device=device, dtype=dtype)
    ssm[0, 1] = -x[2]
    ssm[0, 2] = x[1]
    ssm[1, 0] = x[2]
    ssm[1, 2] = -x[0]
    ssm[2, 0] = -x[1]
    ssm[2, 1] = x[0]
    return ssm


def SO3_exp(theta):
    device = theta.device
    dtype = theta.dtype

    W = skew_sym_mat(theta)
    W2 = W @ W
    angle = torch.norm(theta)
    I = torch.eye(3, device=device, dtype=dtype)
    if angle < 1e-5:
        return I + W + 0.5 * W2
    else:
        return (
            I
            + (torch.sin(angle) / angle) * W
            + ((1 - torch.cos(angle)) / (angle**2)) * W2
        )


def V(theta):
    dtype = theta.dtype
    device = theta.device
    I = torch.eye(3, device=device, dtype=dtype)
    W = skew_sym_mat(theta)
    W2 = W @ W
    angle = torch.norm(theta)
    if angle < 1e-5:
        V = I + 0.5 * W + (1.0 / 6.0) * W2
    else:
        V = (
            I
            + W * ((1.0 - torch.cos(angle)) / (angle**2))
            + W2 * ((angle - torch.sin(angle)) / (angle**3))
        )
    return V


def SE3_exp(tau):
    dtype = tau.dtype
    device = tau.device

    rho = tau[:3]
    theta = tau[3:]
    R = SO3_exp(theta)
    t = V(theta) @ rho

    T = torch.eye(4, device=device, dtype=dtype)
    T[:3, :3] = R
    T[:3, 3] = t
    return T